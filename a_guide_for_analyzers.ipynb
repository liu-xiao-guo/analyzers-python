{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38596026-0a36-4cc4-addd-41b18e0ded82",
   "metadata": {},
   "source": [
    "# Create a connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306783ec-dd14-4fbd-920e-d0d2a9866a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '239980ad27d0', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'GA3py-85TgiwUpINHtUXXQ', 'version': {'number': '8.11.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '64cf052f3b56b1fd4449f5454cb88aca7e739d9a', 'build_date': '2023-12-08T11:33:53.634979452Z', 'build_snapshot': False, 'lucene_version': '9.8.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b0e39-cb2e-4332-af3f-1740f35d0718",
   "metadata": {},
   "source": [
    "# Char map filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720b781-3fb1-429a-bd62-8c65a6f39fc9",
   "metadata": {},
   "source": [
    "## HTML Strip Char Filter (html_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f47d71b-5f88-484b-a2a4-87645008e445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'href',\n",
       " 'http',\n",
       " 'example.com',\n",
       " 'Elasticsearch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"char_filter\": [\"html_strip\"],\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"text\": \"<p>Hello <b>World</b>! This is <a href='<http://example.com>'>Elasticsearch</a>.</p>\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8d56a-6475-4698-89f6-4324764c9be2",
   "metadata": {},
   "source": [
    "## Pattern Replace Char Filter (pattern_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00225a8-8481-4d6e-bf41-e8936cffca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liu', 'xiao', 'guo']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"char_filter\": [\n",
    "            {\n",
    "                \"type\": \"pattern_replace\",\n",
    "                \"pattern\": \"[-_@.]\",  # Removes hyphens, underscores, apostrophes\n",
    "                \"replacement\": \" \"\n",
    "            }\n",
    "        ],\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"text\": \"liu_xiao_guo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548ff94-c220-47fe-9e05-e2fa42fd1c20",
   "metadata": {},
   "source": [
    "## Mapping Char Filter (mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71819325-7701-448a-9dc3-d9fdfa99951d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xiaoguo.liu', 'gives', 'me', 'dollar']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"char_filter\": [\n",
    "            {\n",
    "                \"type\": \"mapping\",\n",
    "                \"mappings\": [\n",
    "                    \"@gmail.com=>\",    # Replace @gmail.com with nothing\n",
    "                    \"$=>dollar\",       # Replace $ with dollar\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"text\": \"xiaoguo.liu@gmail.com gives me $\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ee500-b5d5-4c3a-ab83-3936960d9d9c",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3792836-dffe-43e8-a957-829a319b941b",
   "metadata": {},
   "source": [
    "## Standard Tokenizer (standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e5e9f9-3de2-4120-8f06-2ec2c093cf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '2',\n",
       " 'QUICK',\n",
       " 'Brown',\n",
       " 'Foxes',\n",
       " 'jumps_over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " \"dog's\",\n",
       " 'bone']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3982c-3494-4979-812f-ba4fd68a22bc",
   "metadata": {},
   "source": [
    "## Letter Tokenizer (letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edda78fb-23b6-46ee-9e13-30229c2e998e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'QUICK',\n",
       " 'Brown',\n",
       " 'Foxes',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " 's',\n",
       " 'bone']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"letter\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85136463-8cb8-4cb9-8cdf-4452a7ce55e8",
   "metadata": {},
   "source": [
    "## Lowercase Tokenizer (lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d6d28c-627c-4df5-94f4-dddb5621eab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'foxes',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " 's',\n",
       " 'bone']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"lowercase\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad0ca1-dfe7-4334-ac87-44ea92fe8ca8",
   "metadata": {},
   "source": [
    "## Whitespace Tokenizer (whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728937b5-6596-481e-a667-d6e42567288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '2',\n",
       " 'QUICK',\n",
       " 'Brown-Foxes,',\n",
       " 'jumps_over',\n",
       " 'the',\n",
       " \"lazy-dog's\",\n",
       " 'bone.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"whitespace\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2049ec-a2e5-4244-a520-0775f227fd28",
   "metadata": {},
   "source": [
    "## Classic Tokenizer (classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14704b9c-c88d-4cd0-9b19-b2cc42602114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '2',\n",
       " 'QUICK',\n",
       " 'Brown',\n",
       " 'Foxes',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " \"dog's\",\n",
       " 'bone']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"classic\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb32dfb-f129-426e-8c65-aa7e716c8e5d",
   "metadata": {},
   "source": [
    "## UAX URL Email Tokenizer (uax_url_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec80f341-2094-4f80-b99c-fe6ba558f600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visit',\n",
       " 'https',\n",
       " 'elasticstack.blog.csdn.net',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'best',\n",
       " 'materials',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'Elastic',\n",
       " 'Stack']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"classic\",\n",
    "        \"text\": \"visit https://elasticstack.blog.csdn.net to get the best materials to learn Elastic Stack\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdfa50-bb12-42d7-bd10-aab6852de44a",
   "metadata": {},
   "source": [
    "## N-Gram Tokenizer (ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e61104dc-dfd2-4804-94a2-cd7d3c4e939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hel',\n",
       " 'Hell',\n",
       " 'ell',\n",
       " 'ello',\n",
       " 'llo',\n",
       " 'llo ',\n",
       " 'lo ',\n",
       " 'lo X',\n",
       " 'o X',\n",
       " 'o Xi',\n",
       " ' Xi',\n",
       " ' Xia',\n",
       " 'Xia',\n",
       " 'Xiao',\n",
       " 'iao',\n",
       " 'iaog',\n",
       " 'aog',\n",
       " 'aogu',\n",
       " 'ogu',\n",
       " 'oguo',\n",
       " 'guo']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": {\n",
    "            \"type\": \"ngram\",\n",
    "            \"min_gram\": 3,\n",
    "            \"max_gram\": 4\n",
    "        },\n",
    "        \"text\": \"Hello Xiaoguo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4912be4-0e4d-4fe6-9a24-90d1e9d1ff83",
   "metadata": {},
   "source": [
    "## Edge N-Gram Tokenizer (edge_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "759dd565-17cd-414c-8759-c8f50994daa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QUIC',\n",
       " 'QUICK',\n",
       " 'Brow',\n",
       " 'Brown',\n",
       " 'Foxe',\n",
       " 'Foxes',\n",
       " 'jump',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'lazy',\n",
       " 'bone']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": {\n",
    "                \"type\": \"edge_ngram\",\n",
    "                \"min_gram\": 4,\n",
    "                \"max_gram\": 5,\n",
    "                \"token_chars\": [\"letter\", \"digit\"]\n",
    "        },\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdcf8b-94d3-4c1d-a045-73fa810aaf2e",
   "metadata": {},
   "source": [
    "## Keyword Tokenizer (keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "500637ab-e353-4560-8551-b60d2b2c1f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"keyword\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6ddea-9e46-4b69-ab33-5ad6a87eb14e",
   "metadata": {},
   "source": [
    "## Pattern Tokenizer (pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0968549f-92cd-4320-97a7-9830a656216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'from', 'elasticsearch']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": {\n",
    "          \"type\": \"pattern\",\n",
    "          \"pattern\": \"_+\"\n",
    "        },\n",
    "        \"text\": \"hello_world_from_elasticsearch\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c92244-222e-4de0-8691-32fb33f41f12",
   "metadata": {},
   "source": [
    "## Path Tokenizer (path_hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1559c-f9ba-46c2-bbca-45486dbb520d",
   "metadata": {},
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"path_hierarchy\",\n",
    "        \"text\": \"/usr/local/bin/python\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735846d-2f4b-4661-a356-62107d8ca58f",
   "metadata": {},
   "source": [
    "# Token filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488c042-2037-4c20-a56f-36075995f838",
   "metadata": {},
   "source": [
    "## Apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9f566ff-d446-4ff2-aa53-c81d252b053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '2',\n",
       " 'QUICK',\n",
       " 'Brown',\n",
       " 'Foxes',\n",
       " 'jumps_over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " 'bone']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"filter\": [\"apostrophe\"],\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a73d7d-3b74-406e-9635-babd6c9b4127",
   "metadata": {},
   "source": [
    "## Lowercase Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0909c35d-97be-4f9b-bf11-d3c4d00ae82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"the 2 quick brown-foxes, jumps_over the lazy-dog's bone.\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"filter\": [\"lowercase\"],\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89382495-2f4c-457f-9918-b0c8cbd94d6c",
   "metadata": {},
   "source": [
    "## Uppercase Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd6c4183-4502-43a3-b12b-8bc82982e819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"THE 2 QUICK BROWN-FOXES, JUMPS_OVER THE LAZY-DOG'S BONE.\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"filter\": [\"uppercase\"],\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51425753-74a4-4136-853e-fbb93c94dbca",
   "metadata": {},
   "source": [
    "## Trim Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24ea5d62-44c8-4102-ad37-53dbee409513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"the 2 quick brown-foxes, jumps_over the lazy-dog's bone.\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the text using the custom analyzer\n",
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"keyword\",\n",
    "        \"filter\":[\n",
    "            \"lowercase\",\n",
    "            \"trim\"\n",
    "         ],\n",
    "        \"text\": \" The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone. \"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25dc4da-e987-48d3-949e-aa88eb865552",
   "metadata": {},
   "source": [
    "## ASCII Folding Filter (asciifolding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0522350b-9971-468e-b0ca-b7ccdc97799f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Turkiye']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the text using the custom analyzer\n",
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"filter\": [\"asciifolding\"],\n",
    "        \"text\": \"Türkiye\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cae0f7-b912-4cf9-ad49-8338c8b79060",
   "metadata": {},
   "source": [
    "## Synonym Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0be3c57-29ef-4d55-881a-b9af491a6b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', '2', 'quick', 'brown', 'foxes', 'leap', 'the', 'lazy', \"dog's\", 'bone']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the text using the custom analyzer\n",
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\":[\n",
    "            \"lowercase\",\n",
    "            {\n",
    "              \"type\": \"synonym\",\n",
    "              \"synonyms\": [\"jumps_over => leap\"]\n",
    "            }\n",
    "         ],\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57fd04-fa0b-4550-8680-bf7a6198b16b",
   "metadata": {},
   "source": [
    "## Synonym Graph Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9fb1ad5-4cfb-4826-bf1e-3f08a051ddb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flight',\n",
       " 'from',\n",
       " 'los',\n",
       " 'la',\n",
       " 'angeles',\n",
       " 'to',\n",
       " 'new',\n",
       " 'nyc',\n",
       " 'york',\n",
       " 'city',\n",
       " 'has',\n",
       " 'been',\n",
       " 'delayed',\n",
       " 'by',\n",
       " 'an',\n",
       " 'hour']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\":[\n",
    "            \"lowercase\",\n",
    "            {\n",
    "              \"type\": \"synonym_graph\",\n",
    "              \"synonyms\": [\"NYC, New York City\", \"LA, Los Angeles\"]\n",
    "            }\n",
    "         ],\n",
    "        \"text\": \"Flight from LA to NYC has been delayed by an hour\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37105d04-1da6-40c5-88b6-ffc45b33b206",
   "metadata": {},
   "source": [
    "## Stemmer Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06ae2421-2616-4209-988f-7dfe1da25794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candi', 'ladi', 'plai', 'plai', 'ran', 'run', 'dress']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"stemmer\",\n",
    "            \"language\": \"English\",\n",
    "        },\n",
    "        ],\n",
    "    \"text\": \"candies, ladies, plays, playing, ran, running, dresses\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996376d9-60a4-49f0-8dbb-a40b25c6927d",
   "metadata": {},
   "source": [
    "## KStem Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5916791c-d86d-4657-82fd-54b13fcea6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candy', 'ladies', 'play', 'play', 'ran', 'running']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    " \"tokenizer\": \"standard\",\n",
    " \"filter\": [\n",
    "    'kstem',\n",
    "  ],\n",
    " \"text\": \"candies, ladies, plays, playing, ran, running\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303cdb05-ba3c-451b-b3ef-d17a60be0084",
   "metadata": {},
   "source": [
    "## Porter Stem Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07c74f51-9a0b-49cf-9a70-e285f7c42f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candi', 'ladi', 'plai', 'plai', 'ran', 'run', 'dress']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"pattern_replace\",\n",
    "            \"pattern\": \"[-|.|,]\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"porter_stem\",\n",
    "            \"language\": \"English\",\n",
    "        },\n",
    "        ],\n",
    "    \"text\": \"candies, ladies, plays, playing, ran, running, dresses\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaec56d-85d3-4cf1-a47d-39c6de40e6d1",
   "metadata": {},
   "source": [
    "## Snowball Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a02f747e-7073-45d2-abfb-33b6d9f54298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candies,', 'ladies,', 'plays,', 'playing,', 'ran,', 'running,', 'dress']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "    \"tokenizer\": \"whitespace\",\n",
    "    \"filter\": [\n",
    "        {\n",
    "            \"type\": \"snowball\",\n",
    "            \"language\": \"English\",\n",
    "        },\n",
    "        ],\n",
    "    \"text\": \"candies, ladies, plays, playing, ran, running, dresses\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed55950-67d3-4d5a-993f-2e393db8f8dc",
   "metadata": {},
   "source": [
    "## Stemmer Override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d11807f4-bfda-4b31-8b6e-e53487be24ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candies', 'ladies', 'plays', 'playing', 'ran', 'run', 'dresses']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "      \"tokenizer\": \"standard\",\n",
    "      \"filter\": [\n",
    "        {\n",
    "            \"type\": \"stemmer_override\",\n",
    "            \"language\": \"English\",\n",
    "            \"rules\": [\n",
    "                \"running, runs => run\",\n",
    "                \"stemmer => stemmer\"\n",
    "            ]\n",
    "        },\n",
    "        ],\n",
    "      \"text\": \"candies, ladies, plays, playing, ran, running, dresses\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ebe0f-b60a-4322-94a0-4715000e99e7",
   "metadata": {},
   "source": [
    "## Keyword Marker Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2aebeb5c-166e-4471-9257-aeb3b0abd038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candi', 'ladi', 'plai', 'plai', 'run', 'running']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "     \"tokenizer\": \"whitespace\",\n",
    "     \"filter\": [\n",
    "         {\n",
    "            \"type\": \"keyword_marker\",\n",
    "            \"keywords\": [\"running\"]  # Mark 'running' as a keyword\n",
    "         },\n",
    "         {\n",
    "            \"type\": \"pattern_replace\",\n",
    "            \"pattern\": \"[-|.|,]\"\n",
    "         },\n",
    "         {\n",
    "            \"type\": \"porter_stem\",\n",
    "            \"language\": \"English\",\n",
    "         },\n",
    "      ],\n",
    "     \"text\": \"candies, ladies, plays, playing, runs, running\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd11b94-c333-4c8a-a58f-00f11a1a2ad3",
   "metadata": {},
   "source": [
    "## Stop Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f2d6dc5-4b93-4998-a0b5-5dd4b9bfb2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'sachin', 'I', 'software', 'engineer']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the text using the custom analyzer\n",
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\":{\n",
    "            \"type\":\"stop\",\n",
    "            \"stopwords\": [\"is\",\"am\",\"are\",\"of\",\"if\",\"a\",\"the\"],\n",
    "            \"ignore_case\": True\n",
    "        },\n",
    "        \"text\": \"i am sachin. I Am software engineer.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848993ce-0ebb-4c0d-b487-9eadef3d68d6",
   "metadata": {},
   "source": [
    "## Unique Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f82821f1-d8a2-447c-b5c6-1c76c91e474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'joy']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "     \"tokenizer\": \"whitespace\",\n",
    "     \"filter\":[\n",
    "         \"lowercase\", \"unique\",\n",
    "      ],\n",
    "     \"text\": \"Happy happy joy joy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b38fa-ed38-4427-adb8-b579b27939d6",
   "metadata": {},
   "source": [
    "## Length Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13ac7d3c-f563-47f2-9e1f-011510e24aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', '2', 'the', 'lazy', 'bone']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\":[\n",
    "            \"lowercase\",\n",
    "            {\n",
    "              \"type\": \"length\",\n",
    "              \"min\": 1,\n",
    "              \"max\": 4\n",
    "            }\n",
    "         ],\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3be5b2-2cc7-45f4-94f2-4fd20a640e63",
   "metadata": {},
   "source": [
    "## NGram Token Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f622b6f3-69c4-4616-90da-a67e24bfb93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ski',\n",
       " 'Skin',\n",
       " 'kin',\n",
       " 'kinn',\n",
       " 'inn',\n",
       " 'inny',\n",
       " 'nny',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'lue',\n",
       " 'jea',\n",
       " 'jean',\n",
       " 'ean',\n",
       " 'eans',\n",
       " 'ans',\n",
       " 'lev',\n",
       " 'levi',\n",
       " 'evi',\n",
       " 'evis',\n",
       " 'vis']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "     \"tokenizer\": \"whitespace\",\n",
    "     \"filter\":[\n",
    "         {\n",
    "            \"type\": \"ngram\",\n",
    "            \"min_gram\": 3,\n",
    "            \"max_gram\": 4\n",
    "         }\n",
    "      ],\n",
    "     \"text\": \"Skinny blue jeans by levis\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffe4e7-7e00-4230-8419-f4496e55c344",
   "metadata": {},
   "source": [
    "## Edge NGram Token Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36ceddf2-177b-47a7-98ef-d2b81f775790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ski', 'Skin', 'blu', 'blue', 'jea', 'jean', 'lev', 'levi']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "     \"tokenizer\": \"whitespace\",\n",
    "     \"filter\":[\n",
    "         {\n",
    "            \"type\": \"edge_ngram\",\n",
    "            \"min_gram\": 3,\n",
    "            \"max_gram\": 4\n",
    "         }\n",
    "      ],\n",
    "     \"text\": \"Skinny blue jeans by levis\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tokens\n",
    "[token['token'] for token in response['tokens']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0709c4ce-3cb0-418e-a9fc-32a1b9216469",
   "metadata": {},
   "source": [
    "## Shingle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fe7883c-f954-47bb-a7cc-882974bf2491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'Welcome to',\n",
       " 'Welcome to use',\n",
       " 'to',\n",
       " 'to use',\n",
       " 'to use Elastic',\n",
       " 'use',\n",
       " 'use Elastic',\n",
       " 'use Elastic Stack',\n",
       " 'Elastic',\n",
       " 'Elastic Stack',\n",
       " 'Stack']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "     \"tokenizer\": \"whitespace\",\n",
    "     \"filter\":[\n",
    "        {\n",
    "          \"type\": \"shingle\",\n",
    "          \"min_shingle_size\": 2,\n",
    "          \"max_shingle_size\": 3           \n",
    "        }\n",
    "      ],\n",
    "     \"text\": \"Welcome to use Elastic Stack\"\n",
    "    }\n",
    ")\n",
    "\n",
    "[token['token'] for token in response['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aa1cd-ae16-49eb-9609-9706e27c92fe",
   "metadata": {},
   "source": [
    "# Creating a custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc1767d3-473f-42fe-8457-f97e440bb4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', 'quick', 'brown', 'fox', 'jump', 'over', 'lazi', 'dog', 'bone']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    body={\n",
    "        \"char_filter\": [\n",
    "            {\n",
    "                \"type\": \"mapping\",\n",
    "                \"mappings\": [\n",
    "                    \"- => ' '\", # replacing hyphens with blank space\n",
    "                    \"_ => ' '\", # replacing underscore with blank space\n",
    "                 ]\n",
    "            }\n",
    "        ],\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\": [\"apostrophe\", \"lowercase\", \"stop\", \"porter_stem\"],\n",
    "        \"text\": \"The 2 QUICK Brown-Foxes, jumps_over the lazy-dog's bone.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract and print tokens\n",
    "tokens = [token['token'] for token in response['tokens']]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "41c24f3f-dc67-4663-a85e-a32894788a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_custom_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": {\n",
    "                        \"type\": \"mapping\",\n",
    "                        \"mappings\": [\n",
    "                            \"- => ' '\",\n",
    "                            \"_ => ' '\",\n",
    "                        ]\n",
    "                    },\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\", \"apostrophe\", \"stop\", \"porter_stem\"],\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"routing.allocation.include._tier_preference\": \"data_hot\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\":\"text\", \"analyzer\":\"my_custom_analyzer\"},\n",
    "            \"brand\": {\"type\": \"text\", \"analyzer\":\"my_custom_analyzer\", \"fields\": {\"raw\": {\"type\": \"keyword\"}}},\n",
    "            \"updated_time\": {\"type\": \"date\"}\n",
    "        }\n",
    "}\n",
    "\n",
    "response = es.indices.create(index=\"trial_index\", body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832f18c-527a-4c05-b1f0-ed88d988b272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.11",
   "language": "python",
   "name": "python_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
